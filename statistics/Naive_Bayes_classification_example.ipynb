{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Наивный Байес"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Теорема Байеса\n",
    "$ P(A|B) = \\frac{P(B|A) P(A)}{P(B)} $, где\n",
    "\n",
    "$P(B|A)$ — вероятность наступления события B при истинности гипотезы A;\n",
    "\n",
    "$P(A)$ — априорная вероятность гипотезы A;\n",
    "\n",
    "$P(B)$ —  полная вероятность наступления события B;\n",
    "\n",
    "$P(A|B)$ —  априорная вероятность гипотезы A.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Наивный байесовский классификатор\n",
    "$P(C|D) = \\frac{P(D|C) P(C)}{P(D)}$, где\n",
    "\n",
    "$P(C|D)$ — вероятность что документ $D$ принадлежит классу $C$;\n",
    "\n",
    "$P(D|C)$ — вероятность встретить документ $D$ среди всех документов класса $C$;\n",
    "\n",
    "$P(C)$ — безусловная вероятность встретить документ класса $C$ в корпусе документов (упр. список всех слов);\n",
    "\n",
    "$P(D)$ — безусловная вероятность документа $D$ в корпусе документов.\n",
    "\n",
    "Вероятность $P(D|C)$ можно найти с помощью произведения вероятностей всех слов входящих в документ:\n",
    "$P(D|C) \\approx \\prod_{i=1}^n P(w_{i}|C)$, где\n",
    "\n",
    "$n$ — количество слов;\n",
    "\n",
    "$w_{i}$ — слово $w$ с индексом $i$;\n",
    "\n",
    "$P(w_{i}|C)$ — оценка вероятности встретить слово $w_{i}$ в классе $С$. Существуют различные методы оценки. Например, multinomial bayes model:\n",
    "\n",
    "$P(w_{i}|C) = \\frac{W_{ic}}{W_{c}}$, где\n",
    "\n",
    "$W_{ic}$ — количество вхождения $i$-го слова в документы класса $C$\n",
    "\n",
    "$W_{c}$ — словарь корпуса документов (список всех уникальных слов)\n",
    "\n",
    "Простыми словами:\n",
    "-числитель описывает сколько раз слово встречается в документах класса (включая повторы),\n",
    "-знаменатель – это суммарное количество слов во всех документах этого класса.\n",
    "\n",
    "$P(C) = \\frac{D_{c}}{D}$, где\n",
    "\n",
    "$D_{c}$ — количество документов принадлежащих классу $C$\n",
    "\n",
    "$D$ — общее количество документов в обучающей выборке\n",
    "\n",
    "$P(D) = \\sum_{i=1}^n P(D|C_{i}) P(C_{i})$, ее мы редуцируем т.к. значение неизменно от документа к документу\n",
    "\n",
    "Итоговая формула:\n",
    "\n",
    "$P(C|D) = \\prod_{i=1}^n \\frac{W_{ic}}{\\sum_{i \\in V} W_{ic}} \\frac{D_{c}}{D} = \\frac{D_{c}}{D} \\prod_{i=1}^n \\frac{W_{ic}}{W_{c}}$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Реализуем алгоритм"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/mitya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from numpy import e\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "outputs": [
    {
     "data": {
      "text/plain": "   spam_0        spam_1    ham_0\n0  купить  консультация   купить\n1  виагра        тренер   молоко\n2  дешево     бесплатно  магазин",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>spam_0</th>\n      <th>spam_1</th>\n      <th>ham_0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>купить</td>\n      <td>консультация</td>\n      <td>купить</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>виагра</td>\n      <td>тренер</td>\n      <td>молоко</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>дешево</td>\n      <td>бесплатно</td>\n      <td>магазин</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# создадим словарь корпуса документов\n",
    "docs_corp = pd.DataFrame()\n",
    "docs_corp['spam_0'] = ['купить', 'виагра', 'дешево']\n",
    "docs_corp['spam_1'] = ['консультация', 'тренер', 'бесплатно']\n",
    "docs_corp['ham_0'] = ['купить', 'молоко', 'магазин']\n",
    "docs_corp"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "outputs": [
    {
     "data": {
      "text/plain": "   spam  ham\n0     2    1\n1     6    3",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>spam</th>\n      <th>ham</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_docs = pd.DataFrame()\n",
    "class_docs['spam'] = [len(docs_corp.filter(like='spam').columns), docs_corp.filter(like='spam').count().sum()]\n",
    "class_docs['ham'] = list(docs_corp.filter(like='ham').shape)[::-1]\n",
    "class_docs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "outputs": [
    {
     "data": {
      "text/plain": "3"
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = len(docs_corp.columns)\n",
    "D"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "outputs": [
    {
     "data": {
      "text/plain": "              spam  ham\nwords                  \nбесплатно        1    0\nвиагра           1    0\nдешево           1    0\nконсультация     1    0\nкупить           1    1\nтренер           1    0\nмагазин          0    1\nмолоко           0    1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>spam</th>\n      <th>ham</th>\n    </tr>\n    <tr>\n      <th>words</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>бесплатно</th>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>виагра</th>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>дешево</th>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>консультация</th>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>купить</th>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>тренер</th>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>магазин</th>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>молоко</th>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam = pd.DataFrame(np.unique(docs_corp.filter(like='spam').values, return_counts=True), index=['words', 'spam']).T.set_index(keys=['words'])\n",
    "ham = pd.DataFrame(np.unique(docs_corp.filter(like='ham').values, return_counts=True), index=['words', 'ham']).T.set_index(keys=['words'])\n",
    "words_corp = pd.concat([spam, ham], axis=1).fillna(0)\n",
    "words_corp"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "outputs": [
    {
     "data": {
      "text/plain": "0.6666666666666666"
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_C_spam = class_docs.iat[0, 0] /  D\n",
    "P_C_spam"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "outputs": [
    {
     "data": {
      "text/plain": "0.3333333333333333"
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_C_ham = class_docs.iat[0, 1] /  D\n",
    "P_C_ham"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "outputs": [],
   "source": [
    "# фраза для классификации\n",
    "need_to_class = ['купить', 'молоко']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Вспомним формулу\n",
    "$P(C|D) =  \\frac{D_{c}}{D} \\prod_{i=1}^n \\frac{W_{ic}}{W_{c}}$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.037037037037037035\n"
     ]
    }
   ],
   "source": [
    "P_DC_spam = 1\n",
    "P_DC_ham = 1\n",
    "\n",
    "for word in need_to_class:\n",
    "    P_DC_spam *= words_corp.loc[word][0] / class_docs.iat[1, 0]\n",
    "    P_DC_ham *= words_corp.loc[word][1] / class_docs.iat[1, 1]\n",
    "\n",
    "P_CD_spam = P_C_spam * P_DC_spam\n",
    "P_CD_ham = P_C_ham * P_DC_ham\n",
    "print(P_CD_spam)\n",
    "print(P_CD_ham)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Похоже у нас проблема. Из-за того, что слово 'молоко' не встречается в списке слов из категории спам мы получаем в качестве результата 0.\n",
    "Окей, давайте немного изменим формулу, чтобы решить проблему. Увеличим счетчик каждого слова на 1, вот что получается:\n",
    "\n",
    "$P(w_{i}|C) = \\frac{W_{ic} + 1}{W_{c} + 1}$\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.027210884353741492 0.08333333333333333\n"
     ]
    }
   ],
   "source": [
    "def naive_bayes(need_to_class):\n",
    "    P_DC_spam = 1\n",
    "    P_DC_ham = 1\n",
    "\n",
    "    for word in need_to_class:\n",
    "        P_DC_spam *= (words_corp.loc[word][0] + 1) / (class_docs.iat[1, 0] + 1)\n",
    "        P_DC_ham *= (words_corp.loc[word][1] + 1) / (class_docs.iat[1, 1] + 1)\n",
    "\n",
    "    P_CD_spam = P_C_spam * P_DC_spam\n",
    "    P_CD_ham = P_C_ham * P_DC_ham\n",
    "    return P_CD_spam, P_CD_ham\n",
    "\n",
    "P_CD_spam, P_CD_ham = naive_bayes(need_to_class)\n",
    "print(P_CD_spam, P_CD_ham)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.054421768707482984 0.041666666666666664\n"
     ]
    }
   ],
   "source": [
    "need_to_class = ['купить', 'виагра']\n",
    "P_CD_spam, P_CD_ham = naive_bayes(need_to_class)\n",
    "print(P_CD_spam, P_CD_ham)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Отлично, наш классификатор работает. Давайте избавимся от еще одной возможной проблемы. Если документ будет большим, нам придется перемножать огромное количество небольших чисел, что в итоге приведет к арифметическому переполнению снизу. Воспользуемся свойством произведения логарифма:\n",
    "$log(ab) = log(a)+ log(b)$\n",
    "\n",
    "Модифицируем нашу формулу:\n",
    "\n",
    "$ P(C|D) = log \\frac{D_{c}}{D} + \\sum_{i=1}^n log \\frac{W_{ic}}{W_{c}} $"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "outputs": [],
   "source": [
    "P_C_spam = np.log(class_docs.iat[0, 0] /  D)\n",
    "P_C_ham = np.log(class_docs.iat[0, 1] /  D)\n",
    "\n",
    "\n",
    "def naive_bayes_class(need_to_class):\n",
    "    P_DC_spam = 1\n",
    "    P_DC_ham = 1\n",
    "\n",
    "    for word in need_to_class:\n",
    "        P_DC_spam += np.log((words_corp.loc[word][0] + 1) / (class_docs.iat[1, 0] + 1))\n",
    "        P_DC_ham += np.log((words_corp.loc[word][1] + 1) / (class_docs.iat[1, 1] + 1))\n",
    "\n",
    "    P_CD_spam = P_C_spam + P_DC_spam\n",
    "    P_CD_ham = P_C_ham + P_DC_ham\n",
    "    return P_CD_spam, P_CD_ham"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.604138225658846 -1.4849066497880004\n"
     ]
    }
   ],
   "source": [
    "need_to_class = ['купить', 'молоко']\n",
    "P_CD_spam, P_CD_ham = naive_bayes_class(need_to_class)\n",
    "print(P_CD_spam, P_CD_ham)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv('/Users/mitya/Downloads/spam.csv', encoding='ISO-8859-1')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "outputs": [],
   "source": [
    "train_dataset = train_dataset[['v1', 'v2']]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "outputs": [
    {
     "data": {
      "text/plain": "     v1                                                 v2\n0   ham  Go until jurong point, crazy.. Available only ...\n1   ham                      Ok lar... Joking wif u oni...\n2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n3   ham  U dun say so early hor... U c already then say...\n4   ham  Nah I don't think he goes to usf, he lives aro...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>v1</th>\n      <th>v2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>Go until jurong point, crazy.. Available only ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>Ok lar... Joking wif u oni...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>U dun say so early hor... U c already then say...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives aro...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "outputs": [
    {
     "data": {
      "text/plain": "         D\nham   4825\nspam   747",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>D</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>ham</th>\n      <td>4825</td>\n    </tr>\n    <tr>\n      <th>spam</th>\n      <td>747</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_docs = pd.DataFrame()\n",
    "class_docs['D'] = train_dataset['v1'].value_counts()\n",
    "class_docs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "outputs": [],
   "source": [
    "def text_process(mess):\n",
    "    STOPWORDS = stopwords.words('english') + ['u', 'ü', 'ur', '4', '2', 'im', 'dont', 'doin', 'ure']\n",
    "    nopunc = [char for char in mess if char not in string.punctuation]\n",
    "    nopunc = ''.join(nopunc)\n",
    "    return ' '.join([word for word in nopunc.split() if word.lower() not in STOPWORDS])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "outputs": [],
   "source": [
    "train_dataset['v3'] = train_dataset.v2.apply(text_process)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "outputs": [],
   "source": [
    "words = train_dataset[train_dataset.v1=='ham'].v3.apply(lambda x: [word.lower() for word in x.split()])\n",
    "ham_words = Counter()\n",
    "\n",
    "for msg in words:\n",
    "    ham_words.update(msg)\n",
    "\n",
    "\n",
    "words = train_dataset[train_dataset.v1=='spam'].v3.apply(lambda x: [word.lower() for word in x.split()])\n",
    "spam_words = Counter()\n",
    "\n",
    "for msg in words:\n",
    "    spam_words.update(msg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(ham_words, orient='index')\n",
    "df.columns = ['spam']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "outputs": [
    {
     "data": {
      "text/plain": "        spam   ham\nfree   216.0  59.0\nentry   26.0   0.0\nwkly    14.0   0.0\ncomp    10.0   1.0\nwin     60.0  11.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>spam</th>\n      <th>ham</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>free</th>\n      <td>216.0</td>\n      <td>59.0</td>\n    </tr>\n    <tr>\n      <th>entry</th>\n      <td>26.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>wkly</th>\n      <td>14.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>comp</th>\n      <td>10.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>win</th>\n      <td>60.0</td>\n      <td>11.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam = pd.DataFrame.from_dict(spam_words, orient='index')\n",
    "spam.columns = ['spam']\n",
    "ham = pd.DataFrame.from_dict(ham_words, orient='index')\n",
    "ham.columns = ['ham']\n",
    "words_corp = pd.concat([spam, ham], axis=1).fillna(0)\n",
    "words_corp.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "outputs": [
    {
     "data": {
      "text/plain": "5572"
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = int(class_docs.sum())\n",
    "D"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "outputs": [],
   "source": [
    "P_C_spam = np.log(class_docs.iloc[1] /  D)\n",
    "P_C_ham = np.log(class_docs.iloc[0] /  D)\n",
    "\n",
    "\n",
    "def naive_bayes_class_final(need_to_class):\n",
    "    P_DC_spam = 1\n",
    "    P_DC_ham = 1\n",
    "\n",
    "    for word in need_to_class:\n",
    "        P_DC_spam += np.log((words_corp.loc[word][0] + 1) / (class_docs.iloc[1] + 1))\n",
    "        P_DC_ham += np.log((words_corp.loc[word][1] + 1) / (class_docs.iloc[0]  + 1))\n",
    "\n",
    "    P_CD_spam = P_C_spam + P_DC_spam\n",
    "    P_CD_ham = P_C_ham + P_DC_ham\n",
    "    return P_CD_spam, P_CD_ham"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_CD_spam:  -18.7672534826609\n",
      "P_CD_ham:  -16.428236296624597\n"
     ]
    }
   ],
   "source": [
    "need_to_class = ['still', 'ok', 'call', 'next', 'day']\n",
    "\n",
    "P_CD_spam, P_CD_ham = naive_bayes_class_final(need_to_class)\n",
    "print('P_CD_spam: ', float(P_CD_spam))\n",
    "print('P_CD_ham: ', float(P_CD_ham))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Теперь давайте получим вероятности. Для этого избавимся от логарифмов и нормируем значения:\n",
    "\n",
    "$P(C|D) = \\frac{e^{q_{c}}}{\\sum e^{q_{c}}}$, где\n",
    "$q_{c}$ — логарифмическая оценка"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam_proba:  0.0879427132327269\n",
      "ham_proba 0.9120572867672732\n"
     ]
    }
   ],
   "source": [
    "spam_proba = e**P_CD_spam / (e**P_CD_spam + e**P_CD_ham)\n",
    "ham_proba = e**P_CD_ham / (e**P_CD_spam + e**P_CD_ham)\n",
    "print('spam_proba: ', float(spam_proba))\n",
    "print('ham_proba', float(ham_proba))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
