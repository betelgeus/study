{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/betelgeus/study/blob/master/Transformer_Using_PyTorch_(kaggle%2C_without_train%2C_eval%2C_metrics).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1 align=\"center\" style=\"color:green;font-size: 3em;\" >Реализация трансформера с нуля с помощью PyTorch</h1>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* [1. Введение](#section1)\n",
        "* [2. Импорт библиотек](#section2)\n",
        "* [3. Основные компоненты](#section3)\n",
        "  - [Создание Word Embeddings](#section4)\n",
        "  - [Positional Encoding](#section5)\n",
        "  - [Self Attention](#section6)\n",
        "* [4. Encoder](#section7)\n",
        "* [5. Decoder](#section8)\n",
        "* [6. Тестирование кода](#section9)\n",
        "* [7. Полезные материалы](#section10)\n",
        "\n",
        "\n",
        "<img src=\"https://theaisummer.com/static/6122618d7e1466853e88473ba375cdc7/40ffe/transformer.png\">\n",
        "\n",
        "\n",
        "<a class=\"anchor\" id=\"section1\"></a>\n",
        "<h2 style=\"color:green;font-size: 2em;\">1. Введение</h2>\n",
        "\n",
        "В этом туториале мы поговорим о реализации трансформера из  статьи \"Attention is all you need\" с нуля, используя PyTorch. В основе архитектуры трансформера лежат энкодер и декодер, которые обычно используется для задач машинного перевода.\n",
        "\n",
        "\n",
        "```\n",
        "Примечание: Здесь мы не собираемся углубляться в подробное объяснение трансформеров. Для этого пожалуйста обратитесь к [блогу](http://jalammar.github.io/illustrated-transformer/.) Джея Аламмара. Он дал подробное объяснение внутренней работы трансформеров. Мы сосредоточимся только на технической имплементации.\n",
        "```\n"
      ],
      "metadata": {
        "id": "4BpO2QJIya3Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src = \"https://jalammar.github.io/images/t/The_transformer_encoders_decoders.png\" width=600 height=400>"
      ],
      "metadata": {
        "id": "Hg7AeDDWya3W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "На изображении выше показана модель машинного перевода с французского на английский. Фактически мы можем использовать стек энкодеров (один сверху другого) и стек декодеров, как показано ниже:\n",
        "\n",
        "\n",
        "<img src = \"https://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png\" width=600 height=400>\n",
        "\n",
        "Прежде чем продолжить, давайте взглянем на детальную картину нашей модели.\n",
        "\n",
        "<img src = \"https://miro.medium.com/max/760/1*2vyKzFlzIHfSmOU_lnQE4A.png\" width=350 height=200>"
      ],
      "metadata": {
        "id": "aKdFotEDya3X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a class=\"anchor\" id=\"section2\"></a>\n",
        "<h2 style=\"color:green;font-size: 2em;\">2. Импорт библиотек</h2>"
      ],
      "metadata": {
        "id": "tX-AOoTNya3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# импорт библиотек\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import math,copy,re\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import torchtext\n",
        "import matplotlib.pyplot as plt\n",
        "warnings.simplefilter(\"ignore\")\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-29T11:46:53.419912Z",
          "iopub.execute_input": "2023-08-29T11:46:53.420479Z",
          "iopub.status.idle": "2023-08-29T11:46:56.121482Z",
          "shell.execute_reply.started": "2023-08-29T11:46:53.420442Z",
          "shell.execute_reply": "2023-08-29T11:46:56.120435Z"
        },
        "trusted": true,
        "id": "1gh_ls30ya3Y",
        "outputId": "1cb45af7-ab6f-40b6-f69e-0743dcbe97e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "1.9.1+cpu\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Мы знаем, что для задачи машинного перевода используется архитектура трансформера, включающая энкодер и декодер. Прежде чем переходить к энкодеру или декодеру, давайте обсудим некоторые основные компоненты.\n",
        "\n",
        "<a class=\"anchor\" id=\"section3\"></a>\n",
        "<h2 style=\"color:green;font-size: 2em;\"> Основные компоненты</h2>\n",
        "\n",
        "<a class=\"anchor\" id=\"section4\"></a>\n",
        "<h2 style=\"color:green;\"> Создание Word Embeddings</h2>\n",
        "\n",
        "Прежде всего, нам нужно преобразовать каждое слово во входной последовательности в эмбеддинг. Векторы эмбеддингов создадут более семантическое представление каждого слова.\n",
        "\n",
        "Предположим, что каждый эмбеддинг имеет размерность 512, и предположим, что размер нашего словаря составляет 100. Тогда наша матрица эмбеддингов будет иметь размерность 100x512. Эти матрицы будут использованы в процессе обучения, и во время вывода каждое слово будет сопоставлено соответствующему 512-мерному вектору. Предположим, что у нас есть батч размером 32 и длина последовательности 10 (10 слов). Тогда выход будет иметь размерность 32x10x512."
      ],
      "metadata": {
        "id": "FNxZ2VZMya3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedding(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            vocab_size: размер словаря\n",
        "            embed_dim: размерность эмбеддингов\n",
        "        \"\"\"\n",
        "        super(Embedding, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: входящий вектор\n",
        "        Returns:\n",
        "            out: вектор эмбеддинга\n",
        "        \"\"\"\n",
        "        out = self.embed(x)\n",
        "        return out"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-29T11:46:56.123869Z",
          "iopub.execute_input": "2023-08-29T11:46:56.124220Z",
          "iopub.status.idle": "2023-08-29T11:46:56.131428Z",
          "shell.execute_reply.started": "2023-08-29T11:46:56.124134Z",
          "shell.execute_reply": "2023-08-29T11:46:56.130534Z"
        },
        "trusted": true,
        "id": "xkviitdQya3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a class=\"anchor\" id=\"section5\"></a>\n",
        "<h3 style=\"color:green\">Positional Encoding</h3>\n",
        "\n",
        "Следующий шаг — это позиционное кодирование. Для того чтобы модель могла понимать предложение, ей необходимо знать две вещи о каждом слове:\n",
        "\n",
        "* что означает слово?\n",
        "* какова позиция слова в предложении?\n",
        "\n",
        "В статье \"Attention is all you need\" автор использует следующие функции для создания позиционного кодирования. На нечетных временных шагах используется косинусная функция, а на четных - синусная.\n",
        "\n",
        "$$PE_{(pos, 2i)} = sin(\\frac{pos}{10000^{2i/d_{model}}})$$\n",
        "\n",
        "$$PE_{(pos, 2i+1)} = cos(\\frac{pos}{10000^{2i/d_{model}}})$$\n",
        "\n",
        "\n",
        "```\n",
        "pos -> обозначает порядок в предложении\n",
        "i -> обозначает позицию в измерении вектора вложения\n",
        "\n",
        "```\n",
        "\n",
        "В результате позиционного кодирования мы получим матрицу, аналогичную матрице эмбеддинга. Она будет иметь размерность \"Длина последовательности (предложения)\" X \"Размерность эмбеддинга\". Для каждого токена (слова) в последовательности мы найдем вектор эмбеддинга размерности 1 x 512 и добавим к нему соответствующий позиционный вектор размерностью 1 x 512, чтобы получить выход размерности 1 x 512 для каждого слова/токена.\n",
        "\n",
        "Например, если у нас есть батч размером 32, длина последовательности 10 и размерность эмбединга 512, то у нас будет вектор эмбеддинга размерности 32 x 10 x 512. Точно так же у нас будет вектор позиционного кодирования размерности 32 x 10 x 512. Затем мы получим их сумму.\n",
        "\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/906/1*B-VR6R5vJl3Y7jbMNf5Fpw.png\" height=200 width=400>"
      ],
      "metadata": {
        "id": "Nv4N5B23ya3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Буфер в PyTorch ->\n",
        "# Если у вас есть параметры модели, которые должны быть сохранены и восстановлены в состоянии state_dict,\n",
        "# и не обучаться оптимайзером, вы должны зарегистрировать их как буферы.\n",
        "\n",
        "\n",
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, max_seq_len, embed_model_dim):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            max_seq_len: длина входящей последовательности\n",
        "            embed_model_dim: размерность эмбеддинга\n",
        "        \"\"\"\n",
        "        super(PositionalEmbedding, self).__init__()\n",
        "        self.embed_dim = embed_model_dim\n",
        "\n",
        "        pe = torch.zeros(max_seq_len, self.embed_dim)\n",
        "        for pos in range(max_seq_len):\n",
        "            for i in range(0, self.embed_dim, 2):\n",
        "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/self.embed_dim)))\n",
        "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/self.embed_dim)))\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: входящий вектор\n",
        "        Returns:\n",
        "            x: выход\n",
        "        \"\"\"\n",
        "\n",
        "        # увеличиваем эмбеддинги относительно их размерности\n",
        "        x = x * math.sqrt(self.embed_dim)\n",
        "        # добавляем константу к эмбеддингам\n",
        "        seq_len = x.size(1)\n",
        "        x = x + torch.autograd.Variable(self.pe[:,:seq_len], requires_grad=False)\n",
        "        return x"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-29T11:46:56.132984Z",
          "iopub.execute_input": "2023-08-29T11:46:56.133282Z",
          "iopub.status.idle": "2023-08-29T11:46:56.150940Z",
          "shell.execute_reply.started": "2023-08-29T11:46:56.133244Z",
          "shell.execute_reply": "2023-08-29T11:46:56.149779Z"
        },
        "trusted": true,
        "id": "Yw1GVgF9ya3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a class=\"anchor\" id=\"section6\"></a>\n",
        "<h2 style=\"color:green\"> Self Attention</h2>\n",
        "\n",
        "Давайте поговорим о таких ключевых компонентах трансформеров, как Self Attention and Multihead attention.\n",
        "\n",
        "***Что такое Self Attention?***\n",
        "\n",
        "Предположим, у нас есть последовательность \"Dog is crossing the street because it saw the kitchen\". К чему относится \"it\"? Человек легко поймет, что \"it\" относится к собаке. Для алгоритма машинного обучения не так все очевидно.\n",
        "\n",
        "Когда модель обрабатывает каждое слово, Self Attention позволяет ей \"смотреть\" на другие слова во входной последовательности для поиска подсказок. В результате создается вектор, содержащий зависимости каждого слова от других.\n",
        "\n",
        "Давайте пошагово рассмотрим Self Attention.\n",
        "\n",
        "\n",
        "**Шаг 1:** Первый шаг при вычислении Self Attention - создать три вектора из каждого входного вектора энкодера (в данном случае эмбеддинга каждого слова). Таким образом, для каждого слова мы создаем вектор запроса (Query), вектор ключа (Key) и вектор значения (Value). Каждый из векторов будет иметь размерность 1x64.\n",
        "\n",
        "Поскольку у нас Multihead Attention, у нас будет 8 голов Self Attention.\n",
        "\n",
        "\n",
        "**Как создаются ключи (keys), запросы (queries) и значения (values)?**\n",
        "\n",
        "Для генерации ключей, запросов и значений у нас будет матрица ключей (key matrix), матрица запросов (query matrix) и матрица значений (value matrix). Эти матрицы содержат значения, которые меняются в процессе обучения.\n",
        "\n",
        "```\n",
        "Подсказка для кода:\n",
        "Предположим, у нас есть batch_size=32, sequence_length=10, embedding_dimension=512. После создания эмбеддинга слов и позиционного кодирования наш выход будет размерности 32x10x512.\n",
        "Мы изменяем его размер на 32x10x8x64. (Число 8 - это количество голов в multihead attention. Если не все понятно, не беспокойтесь. Все прояснится, когда взгляните на код.)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "**Шаг 2:** Второй шаг - это расчет оценки (score). То есть мы будем умножать матрицу запросов на матрицу ключей. [Q x K.t]\n",
        "\n",
        "```\n",
        "Подсказка для кода:\n",
        "Предположим, что размерности ключей, запросов и значений составляют 32x10x8x64. Прежде чем продолжить, мы транспонируем каждую из них для удобства умножения (32x8x10x64). Теперь умножаем матрицу запросов на транспонированную матрицу ключей, то есть (32x8x10x64) x (32x8x64x10) -> (32x8x10x10).\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "**Шаг 3:** Теперь делим получившуюся матрицу на квадратный корень из размерности матрицы ключей и применяем к ней функцию Softmax. Так мы получим Scores.\n",
        "\n",
        "```\n",
        "Подсказка для кода:\n",
        "Мы делим вектор размером 32x8x10x10 на 8, то есть на квадратный корень из 64 (размерности матрицы ключей).\n",
        "\n",
        "```\n",
        "\n",
        "**Шаг 4:** Затем умножаем Scores на матрицу значений (value matrix).\n",
        "\n",
        "```\n",
        "Подсказка для кода:\n",
        "После шага 3 наш выход будет иметь размерность 32x8x10x10. Теперь умножьте его на матрицу значений (value matrix) (32x8x10x64), чтобы получить выход размерностью (32x8x10x64). Здесь 8 - это количество голов Attention, а 10 - длина последовательности. Таким образом, для каждого слова у нас есть вектор размерностью 64.\n",
        "\n",
        "```\n",
        "\n",
        "**Шаг 5:** Теперь мы передаем полученные значения через линейный слой, что сформирует выход Multihead Attention.\n",
        "\n",
        "```\n",
        "Подсказка для кода:\n",
        "Вектор (32x8x10x64) транспонируется в (32x10x8x64), а затем изменяется размер на (32x10x512). Затем он передается через линейный слой, чтобы получить вывод размером (32x10x512).\n",
        "\n",
        "```\n",
        "\n",
        "Теперь у нас есть общее представление о том, как работает Multihead Attention. Давайте ознакомимся с его реализацией, чтобы прояснить детали."
      ],
      "metadata": {
        "id": "7YNt3Ezaya3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embed_dim=512, n_heads=8):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            embed_dim: размерность вектора эмбеддинга на выходе\n",
        "            n_heads: количество голов Self Attention\n",
        "        \"\"\"\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "        self.embed_dim = embed_dim    # размерность: 512\n",
        "        self.n_heads = n_heads   # голов: 8\n",
        "        self.single_head_dim = int(self.embed_dim / self.n_heads)   # 512/8 = 64. каждый key, query, value будут размерности 64\n",
        "\n",
        "        #key,query and value matrixes  # 64 x 64\n",
        "        self.query_matrix = nn.Linear(self.single_head_dim , self.single_head_dim ,bias=False)\n",
        "        # одиночная матрица Key для каждого из 8 ключей  # 512x512\n",
        "        self.key_matrix = nn.Linear(self.single_head_dim  , self.single_head_dim, bias=False)\n",
        "        self.value_matrix = nn.Linear(self.single_head_dim ,self.single_head_dim , bias=False)\n",
        "        self.out = nn.Linear(self.n_heads*self.single_head_dim ,self.embed_dim)\n",
        "\n",
        "\n",
        "    def forward(self, key, query, value, mask=None):  # batch_size x sequence_length x embedding_dim    # 32 x 10 x 512\n",
        "        \"\"\"\n",
        "        Args:\n",
        "           key : вектор ключей (key vector)\n",
        "           query : вектор запросов (query vector)\n",
        "           value : вектор значений (value vector)\n",
        "           mask: маска для декодера (mask for decoder)\n",
        "\n",
        "        Returns:\n",
        "           выходной вектор после Multihead Attention\n",
        "        \"\"\"\n",
        "        batch_size = key.size(0)\n",
        "        seq_length = key.size(1)\n",
        "\n",
        "        # размерность запроса (query) может измениться в декодере во время inference.\n",
        "        # поэтому мы не можем использовать общую длину последовательности (seq_length).\n",
        "        seq_length_query = query.size(1)\n",
        "\n",
        "        # 32x10x512\n",
        "        key = key.view(batch_size, seq_length, self.n_heads, self.single_head_dim)  #batch_size x sequence_length x n_heads x single_head_dim = (32x10x8x64)\n",
        "        query = query.view(batch_size, seq_length_query, self.n_heads, self.single_head_dim) #(32x10x8x64)\n",
        "        value = value.view(batch_size, seq_length, self.n_heads, self.single_head_dim) #(32x10x8x64)\n",
        "\n",
        "        k = self.key_matrix(key)       # (32x10x8x64)\n",
        "        q = self.query_matrix(query)\n",
        "        v = self.value_matrix(value)\n",
        "\n",
        "        q = q.transpose(1,2)  # (batch_size, n_heads, seq_len, single_head_dim)    # (32 x 8 x 10 x 64)\n",
        "        k = k.transpose(1,2)  # (batch_size, n_heads, seq_len, single_head_dim)\n",
        "        v = v.transpose(1,2)  # (batch_size, n_heads, seq_len, single_head_dim)\n",
        "\n",
        "        # рассчитаем Attention\n",
        "        # скорректированные ключи (adjust key) используем для матричного умножения\n",
        "        k_adjusted = k.transpose(-1,-2)  # (batch_size, n_heads, single_head_dim, seq_ken)  # (32 x 8 x 64 x 10)\n",
        "        product = torch.matmul(q, k_adjusted)  # (32 x 8 x 10 x 64) x (32 x 8 x 64 x 10) = # (32x8x10x10)\n",
        "\n",
        "        # заполняем те позиции матрицы Product значением (-1e20), где позиции маски равны 0\n",
        "        if mask is not None:\n",
        "             product = product.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "\n",
        "        # делим матрицу Product на квадратный корень размерности отдельной головы (Single Head)\n",
        "        product = product / math.sqrt(self.single_head_dim) # / sqrt(64)\n",
        "\n",
        "        # применяем Softmax\n",
        "        scores = F.softmax(product, dim=-1)\n",
        "\n",
        "        # умножаем на марицу Value\n",
        "        scores = torch.matmul(scores, v)  # (32x8x 10x 10) x (32 x 8 x 10 x 64) = (32 x 8 x 10 x 64)\n",
        "\n",
        "        # сконкатинируем выходы\n",
        "        concat = scores.transpose(1,2).contiguous().view(batch_size, seq_length_query, self.single_head_dim*self.n_heads)  # (32x8x10x64) -> (32x10x8x64)  -> (32,10,512)\n",
        "\n",
        "        output = self.out(concat) #(32,10,512) -> (32,10,512)\n",
        "        return output"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-29T11:46:56.277085Z",
          "iopub.execute_input": "2023-08-29T11:46:56.277452Z",
          "iopub.status.idle": "2023-08-29T11:46:56.300466Z",
          "shell.execute_reply.started": "2023-08-29T11:46:56.277414Z",
          "shell.execute_reply": "2023-08-29T11:46:56.299475Z"
        },
        "trusted": true,
        "id": "l71oBvmMya3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Хорошо, теперь внезапный вопрос может возникнуть у вас в голове. Зачем эта маска используется? Не беспокойтесь, мы разберем это, когда будем говорить о декодере.\n",
        "\n",
        "<a class=\"anchor\" id=\"section7\"></a>\n",
        "<h2 style=\"color:green;font-size: 2em;\"> 4. Encoder</h2>\n",
        "\n",
        "<img src=\"https://www.researchgate.net/profile/Ehsan-Amjadian/publication/352239001/figure/fig1/AS:1033334390013952@1623377525434/Detailed-view-of-a-transformer-encoder-block-It-first-passes-the-input-through-an.jpg\" width=300 height=200>\n",
        "\n",
        "\n",
        "\n",
        "В блоке энкодера происходят следующие преобразования —\n",
        "\n",
        "\n",
        "**Шаг 1:** : Первый вход (дополненные токены, соответствующие предложению) проходит через эмбеддинг слой и слой позиционного кодирования.\n",
        "\n",
        "\n",
        "```\n",
        "Подсказка для кода:\n",
        "предположим, у нас есть на входе матрица размером 32x10 (batch size=32 и sequence length=10). После прохождения через эмбеддинг слой, размер матрицы становится 32x10x512. Затем матрица суммируется с соответствующим вектором позиционного кодирования, что на выходе дает матрицу размером 32x10x512. Затем этот выход передается в слой Multihead Attention.\n",
        "\n",
        "```\n",
        "\n",
        "**Шаг 2:** Как уже обсуждалось выше, дальше данные проходят через слой Multihead Attention, что в результате дает матрицу представлений в качестве выхода.\n",
        "\n",
        "```\n",
        "Подсказка для кода:\n",
        "вход в Multihead Attention будет размером 32x10x512, из которого будут сгенерированы вектора ключей, запросов и значений, как было описано выше, и на выходе мы получаем матрицу размером 32x10x512.\n",
        "\n",
        "```\n",
        "\n",
        "**Шаг 3:** Затем идет нормализация и residual connection (боримся с затуханием градиентов): выход из Multihead Attention добавляется к его входу, затем нормализуется.\n",
        "\n",
        "```\n",
        "Подсказка для кода:\n",
        "выход из Multihead Attention, который имеет размер 32x10x512 суммируется со входом размера 32x10x512 (который является выходом слоя эмбеддингов и позиционного кодирования), затем выход нормализуется.\n",
        "\n",
        "```\n",
        "\n",
        "**Шаг 4:** Затем идет полносвязанный слой (feed forward) и затем слой нормализации с residual connection от входа (вход полносвязанного слоя), куда передается выход после нормализации, и, наконец, получается выход энкодера.\n",
        "\n",
        "\n",
        "```\n",
        "Подсказка для кода:\n",
        "Нормализованный выход будет иметь размерность 32x10x512. Он проходит через 2 линейных слоя: 32x10x512 -> 32x10x2048 -> 32x10x512. Наконец, есть residual connection, которые добавляется к выходу, и выход слоя нормализуется. Таким образом, создается вектор размером 32x10x512 в качестве выхода для энкодера.\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "ZuN9EwtRya3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, expansion_factor=4, n_heads=8):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        \"\"\"\n",
        "        Args:\n",
        "           embed_dim: размерность эмбеддингов\n",
        "           expansion_factor: фактор, определяющий размерность выхода линейного слоя\n",
        "           n_heads: количество Attention Heads\n",
        "\n",
        "        \"\"\"\n",
        "        self.attention = MultiHeadAttention(embed_dim, n_heads)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        self.feed_forward = nn.Sequential(\n",
        "                          nn.Linear(embed_dim, expansion_factor*embed_dim),\n",
        "                          nn.ReLU(),\n",
        "                          nn.Linear(expansion_factor*embed_dim, embed_dim)\n",
        "        )\n",
        "\n",
        "        self.dropout1 = nn.Dropout(0.2)\n",
        "        self.dropout2 = nn.Dropout(0.2)\n",
        "\n",
        "\n",
        "    def forward(self,key,query,value):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "           key : вектор ключей (key vector)\n",
        "           query : вектор запросов (query vector)\n",
        "           value : вектор значений (value vector)\n",
        "           norm2_out: выход TransformerBlock\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        attention_out = self.attention(key,query,value)  # 32x10x512\n",
        "        attention_residual_out = attention_out + value  # 32x10x512\n",
        "        norm1_out = self.dropout1(self.norm1(attention_residual_out)) # 32x10x512\n",
        "\n",
        "        feed_fwd_out = self.feed_forward(norm1_out) # 32x10x512 -> # 32x10x2048 -> 32x10x512\n",
        "        feed_fwd_residual_out = feed_fwd_out + norm1_out # 32x10x512\n",
        "        norm2_out = self.dropout2(self.norm2(feed_fwd_residual_out)) # 32x10x512\n",
        "        return norm2_out\n",
        "\n",
        "\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        seq_len : длина входящей последовательности\n",
        "        embed_dim: размерность эмбеддингов\n",
        "        num_layers: количество слоев энкодера\n",
        "        expansion_factor: фактор, определяющий размерность выхода линейного полносвязанного слоя\n",
        "        n_heads: количество голов в Multihead Attention\n",
        "\n",
        "    Returns:\n",
        "        out: выход Encoder'а\n",
        "    \"\"\"\n",
        "    def __init__(self, seq_len, vocab_size, embed_dim, num_layers=2, expansion_factor=4, n_heads=8):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "\n",
        "        self.embedding_layer = Embedding(vocab_size, embed_dim)\n",
        "        self.positional_encoder = PositionalEmbedding(seq_len, embed_dim)\n",
        "\n",
        "        self.layers = nn.ModuleList([TransformerBlock(embed_dim, expansion_factor, n_heads) for i in range(num_layers)])\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        embed_out = self.embedding_layer(x)\n",
        "        out = self.positional_encoder(embed_out)\n",
        "        for layer in self.layers:\n",
        "            out = layer(out,out,out)\n",
        "\n",
        "        return out  # 32x10x512"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-29T11:46:57.185223Z",
          "iopub.execute_input": "2023-08-29T11:46:57.185526Z",
          "iopub.status.idle": "2023-08-29T11:46:57.204622Z",
          "shell.execute_reply.started": "2023-08-29T11:46:57.185493Z",
          "shell.execute_reply": "2023-08-29T11:46:57.203432Z"
        },
        "trusted": true,
        "id": "7f_5onzIya3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a class=\"anchor\" id=\"section8\"></a>\n",
        "<h2 style=\"color:green;font-size: 2em;\"> 5. Decoder</h2>\n",
        "\n",
        "<img src=\"https://discuss.pytorch.org/uploads/default/optimized/3X/8/e/8e5d039948b8970e6b25395cb207febc82ba320a_2_177x500.png\" height=100 width=250>\n",
        "\n",
        "Вот мы и рассмотрели большую часть компонентов энкодера. Давайте перейдем к компонентам декодера. Мы будем использовать выход энкодера для создания векторов ключей и значений для декодера. В декодере есть два вида Multuhead Attention: одон внутри декодера и другое между энкодером и декодером. Не беспокойтесь, мы пойдем шаг за шагом."
      ],
      "metadata": {
        "id": "NbN4X38_ya3h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Давайте детально рассмотрим фазу обучения.\n",
        "\n",
        "**Шаг 1:** Первым шагом целевая последовательность (таргет) передается через эмбеддинг слой  и слой позиционного кодирования для создания вектора эмбеддинга размерностью 1x512 для каждого слова в целевой последовательности.\n",
        "\n",
        "\n",
        "```\n",
        "Подсказка для кода:\n",
        "Предположим, что длина последовательности составляет 10, размер батча - 32, а размерность вектора эмбеддинга - 512. У нас есть вход размером 32x10 для матрицы эмбеддингов, которая дает выход размером 32x10x512, который добавляется к позиционному кодированию той же размерности, и создается выход размером 32x10x512.\n",
        "```\n",
        "\n",
        "**Шаг 2:** Выход после эмбеддинг слоя  проходит через слои Multihead Attention, как и раньше (создание матриц: ключей, запросов и значений из таргета). В результате прохождения через Multihead Attention мы получаем вектор, который содержит информацию о взаимосвязи (внимании) слов между собой. В этот раз основное отличие заключается в том, что мы используем маску.\n",
        "\n",
        "**Зачем нужна маска?**\n",
        "\n",
        "Маска нужна, чтобы слово не смотрело на будущие слова при проверки зависимости. Поскольку мы создаем Attention для слов в целевой последовательности, нам не нужно, чтобы определенное слово видело будущие слова. Например, в фразе \"Я студент\", нам не нужно, чтобы слово \"Я\" видело слово \"студент\".\n",
        "\n",
        "\n",
        "```\n",
        "Подсказка для кода:\n",
        "Для создания Attention мы создали треугольную матрицу с 1 и 0. Например, треугольная матрица для длины последовательности 5 выглядит следующим образом:\n",
        "\n",
        "1 0 0 0 0\n",
        "1 1 0 0 0\n",
        "1 1 1 0 0\n",
        "1 1 1 1 0\n",
        "1 1 1 1 1\n",
        "\n",
        "После умножения ключей (keys) на запросы (values) мы заполняем все позиции со значением 0 отрицательной бесконечностью. В коде мы заполняем это очень маленьким числом, чтобы избежать ошибок деления (с помощью -1e20).\n",
        "```\n",
        "\n",
        "**Шаг 3:** Как и раньше, у нас есть слои Add и Norm, где выход эмбеддинг слоя добавляется к результату Attentions и нормализуется.\n",
        "\n",
        "\n",
        "\n",
        "**Шаг 4:** Затем у нас есть еще один слой Multihead Attention, а затем слой Add и Norm. Это Multihead Attention называется Multihead Attention между декодером и энкодером. Для этого Multihead Attention мы создаем векторы ключей (keys) и  значений (values) из выхода энкодера. Запрос (query) создается из выхода предыдущего слоя декодера.\n",
        "\n",
        "```\n",
        "Подсказка для кода:\n",
        "Таким образом, у нас есть выход размером 32x10x512 из энкодера. Ключи и значения для всех слов создаются из него. Точно так же матрица запросов создается из выхода предыдущего слоя декодера (32x10x512).\n",
        "```\n",
        "\n",
        "Затем выход проходит через слой Multihead Attention (мы используем количество голов = 8), затем через слой Add и Norm. Здесь вывод из предыдущего слоя энкодера (то есть выход из предыдущего слоя Add и Norm) добавляется к выходу между декодером и энкодером и затем нормализуется.\n",
        "\n",
        "\n",
        "**Шаг 5:** Далее у нас есть полносвязанный линейный слой  (feed forward) с Add и Norm, который аналогичен тому, что есть в энкодере.\n",
        "\n",
        "\n",
        "**Шаг 6:** Наконец, мы создаем линейный слой длиной, равной количеству слов в общем целевой корпусе, и добавляем к нему функцию softmax, чтобы получить вероятность для каждого слова."
      ],
      "metadata": {
        "id": "ZvzMqSgPya3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, expansion_factor=4, n_heads=8):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        \"\"\"\n",
        "        Args:\n",
        "           embed_dim: размерность эмбеддингов\n",
        "           expansion_factor: фактор, определяющий размерность выхода линейного полносвязанного слоя\n",
        "           n_heads: количество Attention Head\n",
        "\n",
        "        \"\"\"\n",
        "        self.attention = MultiHeadAttention(embed_dim, n_heads=8)\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.transformer_block = TransformerBlock(embed_dim, expansion_factor, n_heads)\n",
        "\n",
        "\n",
        "    def forward(self, key, query, x, mask):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "           key : вектор ключей (key vector)\n",
        "           query : вектор запросов (query vector)\n",
        "           value : вектор значений (value vector)\n",
        "           mask: маска для декодера (mask for decoder)\n",
        "           key: key vector\n",
        "           query: query vector\n",
        "           value: value vector\n",
        "           mask: маска для Multihead Attention\n",
        "        Returns:\n",
        "           out: выход TransformerBlock\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # мы должны использовать маску только для первого Attention\n",
        "        attention = self.attention(x, x, x, mask=mask) # 32x10x512\n",
        "        value = self.dropout(self.norm(attention + x))\n",
        "\n",
        "        out = self.transformer_block(key, query, value)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, target_vocab_size, embed_dim, seq_len, num_layers=2, expansion_factor=4, n_heads=8):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        \"\"\"\n",
        "        Args:\n",
        "           target_vocab_size: размерность корпуса документов таргета\n",
        "           embed_dim: размерность эмбеддингов\n",
        "           seq_len : длина входящей последовательности\n",
        "           num_layers: количество слоев Encoder'а\n",
        "           expansion_factor: фактор, который определяет количество слоев в полносвязанных линейных слоях\n",
        "           n_heads: количество голов in Multihead Attention\n",
        "\n",
        "        \"\"\"\n",
        "        self.word_embedding = nn.Embedding(target_vocab_size, embed_dim)\n",
        "        self.position_embedding = PositionalEmbedding(seq_len, embed_dim)\n",
        "\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                DecoderBlock(embed_dim, expansion_factor=4, n_heads=8)\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "\n",
        "        )\n",
        "        self.fc_out = nn.Linear(embed_dim, target_vocab_size)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "\n",
        "    def forward(self, x, enc_out, mask):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: таргетный вектор, который подается на вход модели\n",
        "            enc_out : выходной вектор Encoder'а\n",
        "            trg_mask: маска для Decoder'a Self Attention\n",
        "        Returns:\n",
        "            out: вектор выхода\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        x = self.word_embedding(x)  # 32x10x512\n",
        "        x = self.position_embedding(x) # 32x10x512\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(enc_out, x, enc_out, mask)\n",
        "\n",
        "        out = F.softmax(self.fc_out(x))\n",
        "        return out"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-29T11:46:58.520072Z",
          "iopub.execute_input": "2023-08-29T11:46:58.520438Z",
          "iopub.status.idle": "2023-08-29T11:46:58.540851Z",
          "shell.execute_reply.started": "2023-08-29T11:46:58.520401Z",
          "shell.execute_reply": "2023-08-29T11:46:58.539675Z"
        },
        "trusted": true,
        "id": "Zfa2w-Isya3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Наконец, мы упорядочим все подмодули и создадим полную архитектуру трансформера."
      ],
      "metadata": {
        "id": "dlzAIG-4ya3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, embed_dim, src_vocab_size, target_vocab_size, seq_length, num_layers=2, expansion_factor=4, n_heads=8):\n",
        "        super(Transformer, self).__init__()\n",
        "        \"\"\"\n",
        "        Args:\n",
        "           embed_dim:  размерность эмбеддингов\n",
        "           src_vocab_size: размер корпуса документов source\n",
        "           target_vocab_size: размер корпуса документов target\n",
        "           seq_length : длина входящей последовательности\n",
        "           num_layers: количестов  слоев Encoder'а\n",
        "           expansion_factor: фактор, который определяет количество слоев в полносвязанных линейных слоях\n",
        "           n_heads: количество голов in Multihead Attention\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        self.target_vocab_size = target_vocab_size\n",
        "\n",
        "        self.encoder = TransformerEncoder(seq_length, src_vocab_size, embed_dim, num_layers=num_layers, expansion_factor=expansion_factor, n_heads=n_heads)\n",
        "        self.decoder = TransformerDecoder(target_vocab_size, embed_dim, seq_length, num_layers=num_layers, expansion_factor=expansion_factor, n_heads=n_heads)\n",
        "\n",
        "\n",
        "    def make_trg_mask(self, trg):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            trg: последовательность target'а\n",
        "        Returns:\n",
        "            trg_mask: маска target'а\n",
        "        \"\"\"\n",
        "        batch_size, trg_len = trg.shape\n",
        "        # возвращает нижнюю треугольную часть матрицы, заполненную единицами\n",
        "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
        "            batch_size, 1, trg_len, trg_len\n",
        "        )\n",
        "        return trg_mask\n",
        "\n",
        "\n",
        "    def decode(self,src,trg):\n",
        "        \"\"\"\n",
        "        для инференса\n",
        "        Args:\n",
        "            src: вход в Encoder\n",
        "            trg: вход в Decoder\n",
        "        out:\n",
        "            out_labels : возвращает итоговый предикт последовательности\n",
        "        \"\"\"\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "        enc_out = self.encoder(src)\n",
        "        out_labels = []\n",
        "        batch_size,seq_len = src.shape[0],src.shape[1]\n",
        "        # outputs = torch.zeros(seq_len, batch_size, self.target_vocab_size)\n",
        "        out = trg\n",
        "\n",
        "        for i in range(seq_len): # 10\n",
        "            out = self.decoder(out,enc_out,trg_mask) #bs x seq_len x vocab_dim\n",
        "            # возьмем последний токен\n",
        "            out = out[:,-1,:]\n",
        "\n",
        "            out = out.argmax(-1)\n",
        "            out_labels.append(out.item())\n",
        "            out = torch.unsqueeze(out,axis=0)\n",
        "\n",
        "        return out_labels\n",
        "\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            src: вход в Encoder\n",
        "            trg: вход в Decoder\n",
        "        out:\n",
        "            out: итоговый вектор с вероятностями каждого таргетного слова\n",
        "        \"\"\"\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "        enc_out = self.encoder(src)\n",
        "\n",
        "        outputs = self.decoder(trg, enc_out, trg_mask)\n",
        "        return outputs"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-29T11:46:59.562198Z",
          "iopub.execute_input": "2023-08-29T11:46:59.562492Z",
          "iopub.status.idle": "2023-08-29T11:46:59.579892Z",
          "shell.execute_reply.started": "2023-08-29T11:46:59.562460Z",
          "shell.execute_reply": "2023-08-29T11:46:59.578515Z"
        },
        "trusted": true,
        "id": "eLJuE2hhya3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a class=\"anchor\" id=\"section9\"></a>\n",
        "<h2 style=\"color:green;font-size: 2em;\"> 6. Тестирование кода </h2>\n",
        "\n",
        "Предположим, у нас есть входная последовательность длиной 10 и целевая последовательность длиной 10."
      ],
      "metadata": {
        "id": "7Ns7aHMVya3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src_vocab_size = 11\n",
        "target_vocab_size = 11\n",
        "num_layers = 6\n",
        "seq_length= 12\n",
        "\n",
        "\n",
        "# пусть 0 будет sos токеном и 1 будет eos токеном\n",
        "src = torch.tensor([[0, 2, 5, 6, 4, 3, 9, 5, 2, 9, 10, 1],\n",
        "                    [0, 2, 8, 7, 3, 4, 5, 6, 7, 2, 10, 1]])\n",
        "target = torch.tensor([[0, 1, 7, 4, 3, 5, 9, 2, 8, 10, 9, 1],\n",
        "                       [0, 1, 5, 6, 2, 4, 7, 6, 2, 8, 10, 1]])\n",
        "\n",
        "print(src.shape,target.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-29T11:47:01.136231Z",
          "iopub.execute_input": "2023-08-29T11:47:01.136531Z",
          "iopub.status.idle": "2023-08-29T11:47:01.152225Z",
          "shell.execute_reply.started": "2023-08-29T11:47:01.136498Z",
          "shell.execute_reply": "2023-08-29T11:47:01.150681Z"
        },
        "trusted": true,
        "id": "9UR38VJaya3k",
        "outputId": "861a1533-9c2d-4279-bdbb-2df97ad78706"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "torch.Size([2, 12]) torch.Size([2, 12])\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Transformer(embed_dim=512, src_vocab_size=src_vocab_size,\n",
        "                    target_vocab_size=target_vocab_size, seq_length=seq_length,\n",
        "                    num_layers=num_layers, expansion_factor=4, n_heads=8)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-29T11:47:02.194346Z",
          "iopub.execute_input": "2023-08-29T11:47:02.194680Z",
          "iopub.status.idle": "2023-08-29T11:47:02.676661Z",
          "shell.execute_reply.started": "2023-08-29T11:47:02.194643Z",
          "shell.execute_reply": "2023-08-29T11:47:02.675856Z"
        },
        "trusted": true,
        "id": "ed8e0upxya3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "wqeuHl8Pya3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = model(src, target)\n",
        "out.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-29T11:47:16.215759Z",
          "iopub.execute_input": "2023-08-29T11:47:16.216091Z",
          "iopub.status.idle": "2023-08-29T11:47:16.363109Z",
          "shell.execute_reply.started": "2023-08-29T11:47:16.216059Z",
          "shell.execute_reply": "2023-08-29T11:47:16.362165Z"
        },
        "trusted": true,
        "id": "g8tuMjasya3l",
        "outputId": "808b109d-b54a-4d61-b94c-ad8454b53aad"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 11,
          "output_type": "execute_result",
          "data": {
            "text/plain": "torch.Size([2, 12, 11])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# инференс\n",
        "src = torch.tensor([[0, 2, 5, 6, 4, 3, 9, 5, 2, 9, 10, 1]])\n",
        "trg = torch.tensor([[0]])\n",
        "print(src.shape,trg.shape)\n",
        "out = model.decode(src, trg)\n",
        "out"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-29T10:47:11.959209Z",
          "iopub.execute_input": "2023-08-29T10:47:11.959953Z",
          "iopub.status.idle": "2023-08-29T10:47:12.607110Z",
          "shell.execute_reply.started": "2023-08-29T10:47:11.959885Z",
          "shell.execute_reply": "2023-08-29T10:47:12.606323Z"
        },
        "trusted": true,
        "id": "txkARYTuya3m",
        "outputId": "1978de64-b94c-4487-d31b-ed56830b05a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "torch.Size([1, 12]) torch.Size([1, 1])\n",
          "output_type": "stream"
        },
        {
          "execution_count": 23,
          "output_type": "execute_result",
          "data": {
            "text/plain": "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Важно: В коде реализован только feed forward.**"
      ],
      "metadata": {
        "id": "GDMQ5WBLya3n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<a class=\"anchor\" id=\"section10\"></a>\n",
        "<h2 style=\"color:green;font-size: 2em;\"> 10.  Полезные материалы</h2>\n",
        "\n",
        "* Understanding transformers\n",
        "  - https://theaisummer.com/transformer/\n",
        "  - https://jalammar.github.io/illustrated-transformer/\n",
        "* Pytorch implementation\n",
        "  - https://www.youtube.com/watch?v=U0s0f995w14"
      ],
      "metadata": {
        "id": "kSBLSE_lya3n"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tQ3dJmVxya3o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}